{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVz1A_-srqo0"
   },
   "source": [
    "# Assignment 3\n",
    "# CSCI-P556, Spring 2019\n",
    "## Due date: Monday, March 25, 2019, by 11:59PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bT2n-ypxrwhb"
   },
   "source": [
    "## Problem 1: Building a Neural Networks using Tensorflow's Low-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCmIcvmYuAfH"
   },
   "source": [
    "The most popular deep learning library is Tensorflow. This library offers two ways develop deep learning models, these are through the High-Level API (Keras) and the Low-Level API. Keras' API is really straight-forward, below we include [Tensorflow's example](https://www.tensorflow.org/tutorials/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKlB49oqq_p5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8vaIneq4rMjA",
    "outputId": "5af1b82a-499f-44f3-87b9-82bb96547f85"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "8keTVIJSrQR1",
    "outputId": "b1725ab5-501b-4a4a-a0f3-b51829da103f"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "EKUiE0_hrRdF",
    "outputId": "c0b03f88-6737-4e33-fdfb-38359ff5117b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 15s 254us/sample - loss: 0.2190 - acc: 0.9349\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 15s 258us/sample - loss: 0.0982 - acc: 0.9690\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 16s 266us/sample - loss: 0.0677 - acc: 0.9783\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 16s 267us/sample - loss: 0.0531 - acc: 0.9831\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 16s 271us/sample - loss: 0.0438 - acc: 0.9854\n",
      "10000/10000 [==============================] - 1s 67us/sample - loss: 0.0666 - acc: 0.9803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06657557066550653, 0.9803]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKHdfXkEt_xk"
   },
   "source": [
    "Easy, right? Well, we are going to get our hands dirty with the low-level API. **Your task for this problem is to implement a fully-connected deep neural network using [Tensorflow's low-level API](https://www.tensorflow.org/guide).** Make sure that you code can handle passing parameters for the number of layers and hidden units, as well as any other parameter that you deem necessary.\n",
    "\n",
    "Note: we will take into account your models' accuracy when we grade. For example, if the average accuracy in the class is 91% accuracy and your model is giving 70% accuracy, then you did something wrong. If you aren't getting a good accuracy, check if your loss function is the correct one or try tweaking the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from __future__ import print_function\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vjkri\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232365\n",
      "294000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Used to mount drive in Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "'''\n",
    "### Loading and preprocessing Kuzushiji-49 dataset ###\n",
    "\n",
    "train_x = np.load(r'C:\\Users\\vjkri\\Desktop\\AML\\kmnist\\k49-train-imgs.npz')['arr_0']\n",
    "train_y = np.load(r'C:\\Users\\vjkri\\Desktop\\AML\\kmnist\\k49-train-labels.npz')['arr_0']\n",
    "test_x = np.load(r'C:\\Users\\vjkri\\Desktop\\AML\\kmnist\\k49-test-imgs.npz')['arr_0']\n",
    "test_y = np.load(r'C:\\Users\\vjkri\\Desktop\\AML\\kmnist\\k49-test-labels.npz')['arr_0']\n",
    "\n",
    "img_width = train_x[0].shape[0]\n",
    "img_height = train_x[0].shape[1]\n",
    "\n",
    "train_x = [train_x[i].flatten() for i in range(len(train_x))]\n",
    "test_x = [test_x[i].flatten() for i in range(len(test_x))]\n",
    "\n",
    "#train_x = normalize(train_x, norm = 'l2')\n",
    "#test_x = normalize(test_x, norm = 'l2')\n",
    "\n",
    "train_x, test_x = np.array(train_x), np.array(test_x)\n",
    "\n",
    "train_y = np.reshape(train_y,(train_y.shape[0],1))\n",
    "test_y = np.reshape(test_y,(test_y.shape[0],1))\n",
    "\n",
    "#Oversampling minority classses\n",
    "ros = RandomOverSampler(random_state = 0)\n",
    "train_x_ros, train_y_ros = ros.fit_resample(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode for neural nets\n",
    "op_train, op_test = [], []\n",
    "\n",
    "for each in train_y_ros:\n",
    "    temp = np.zeros((49))\n",
    "    temp[each] = 1\n",
    "    op_train.append(temp)\n",
    "for each in test_y:\n",
    "    temp = np.zeros((49))\n",
    "    temp[each] = 1\n",
    "    op_test.append(temp)\n",
    "    \n",
    "train_y_nn = np.array(op_train)\n",
    "test_y_nn = np.array(op_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading and preprocessing Kuzushiji Kanji dataset ###\n",
    "import cv2\n",
    "#give input according to the location of dataset\n",
    "dir_path = r'C:\\Users\\vjkri\\Desktop\\AML\\kmnist\\kkanji\\kkanji2'\n",
    "dirs = os.listdir(dir_path)\n",
    "kanji_df = pd.DataFrame(columns = ('X','Y'))\n",
    "\n",
    "for name in dirs:\n",
    "    each_class = os.path.join(dir_path,name)\n",
    "    class_dir = os.listdir(each_class)\n",
    "    for images in class_dir:\n",
    "        img_loc = os.path.join(dir_path,name,images)\n",
    "        img = cv2.imread(img_loc, cv2.IMREAD_GRAYSCALE)\n",
    "        img = img.flatten()\n",
    "        kanji_df = kanji_df.append({'X' : img, 'Y' : name}, ignore_index = True)\n",
    "\n",
    "kanji_df = kanji_df.sample(frac = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_kanji = kanji_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_x_arr = np.array(list(kanji_df['X']))\n",
    "kanji_y_arr = np.array(list(kanji_df['Y']))\n",
    "\n",
    "#Oversampling minority classes \n",
    "ros = RandomOverSampler(random_state = 0)\n",
    "kanji_x_ros, kanji_y_ros = ros.fit_resample(kanji_x_arr, kanji_y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take 20% of dataset as test set\n",
    "kanji_train_x, kanji_train_y, kanji_test_x, kanji_test_y = train_test_split(kanji_x_ros, kanji_y_ros, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode for neural nets\n",
    "op_train, op_test = [], []\n",
    "\n",
    "for each in kanji_train_y:\n",
    "    temp = np.zeros((3832))\n",
    "    temp[each] = 1\n",
    "    op_train.append(temp)\n",
    "for each in kanji_test_y:\n",
    "    temp = np.zeros((3832))\n",
    "    temp[each] = 1\n",
    "    op_test.append(temp)\n",
    "    \n",
    "kanji_train_y_nn = np.array(op_train)\n",
    "kanji_test_y_nn = np.array(op_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "138XiGKQxK1h"
   },
   "outputs": [],
   "source": [
    "# Your neural network implementation goes here\n",
    "\n",
    "def neural_net(train_x, train_y, test_x, test_y, learning_rate = 0.01, max_epochs = 30, pixel_dim = 784, classes = 49):\n",
    "    learning_rate = learning_rate\n",
    "    max_epochs = max_epochs\n",
    "    n_hidden_1 = pixel_dim\n",
    "    n_hidden_2 = 512\n",
    "    n_hidden_3 = 512\n",
    "    n_hidden_4 = 512\n",
    "    n_hidden_5 = 512\n",
    "    classes = 49\n",
    "    \n",
    "    X = tf.placeholder('float', [None, pixel_dim])\n",
    "    Y = tf.placeholder('float', [None, classes])\n",
    "\n",
    "    W1 = tf.Variable(tf.random_normal([pixel_dim, n_hidden_1]))\n",
    "    W2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
    "    W3 = tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3]))\n",
    "    W4 = tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4]))\n",
    "    W5 = tf.Variable(tf.random_normal([n_hidden_4, n_hidden_5]))\n",
    "    Woutput = tf.Variable(tf.random_normal([n_hidden_5, classes]))\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([n_hidden_1]))\n",
    "    b2 = tf.Variable(tf.zeros([n_hidden_2]))\n",
    "    b3 = tf.Variable(tf.zeros([n_hidden_3]))\n",
    "    b4 = tf.Variable(tf.zeros([n_hidden_4]))\n",
    "    b5 = tf.Variable(tf.zeros([n_hidden_5]))\n",
    "    boutput = tf.Variable(tf.zeros([classes]))\n",
    "\n",
    "    layer1 = tf.add(tf.matmul(X,W1),b1)\n",
    "    #layer1 = tf.nn.relu(layer1)\n",
    "    layer2 = tf.add(tf.matmul(layer1,W2),b2)\n",
    "     #layer2 = tf.nn.relu(layer2)\n",
    "    layer3 = tf.add(tf.matmul(layer2,W3),b3)\n",
    "     #layer3 = tf.nn.relu(layer3)\n",
    "    layer4 = tf.add(tf.matmul(layer3,W4),b4)\n",
    "     #layer4 = tf.nn.relu(layer4)\n",
    "    layer5 = tf.add(tf.matmul(layer4,W5),b5)\n",
    "    output_layer = tf.add(tf.matmul(layer5,Woutput),boutput)\n",
    "\n",
    "    logits = output_layer\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "\n",
    "    optimize = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "    predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        epochs = 1\n",
    "        while epochs<=max_epochs:\n",
    "            sess.run(optimize, feed_dict = {X: train_x, Y: train_y})\n",
    "            loss_ = sess.run(loss, feed_dict = {X: train_x, Y: train_y})\n",
    "            accuracy_ = sess.run(accuracy, feed_dict = {X: train_x, Y: train_y})\n",
    "            print('Epoch', epochs)\n",
    "            #print('Loss', loss_)\n",
    "            print('Accuracy', accuracy_)\n",
    "            epochs += 1\n",
    "            test_acc = sess.run(accuracy, feed_dict = {X: test_x, Y: test_y})\n",
    "            print('Test accuracy', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-g7QFzsxVrz"
   },
   "source": [
    "We will be using two versions of [Kuzushiji dataset](https://arxiv.org/abs/1812.01718) to test your models: Kuzushiji-49 and Kuzushiji-Kanji. The descriptions of datasets are:\n",
    "\n",
    "> \"Kuzushiji-49, as the name suggests, has 49 classes (28x28 grayscale, 270,912 images), is a much larger, but imbalanced dataset containing 48 Hiragana characters and one Hiragana iteration mark.\"\n",
    "\n",
    "> \"Kuzushiji-Kanji is an imbalanced dataset of total 3832 Kanji characters (64x64 grayscale, 140,426 images), ranging from 1,766 examples to only a single example per class.\"\n",
    "\n",
    "If you read the description carefully, you can notice that these aren't datasets that we can just feed to the model without doing some pre-processing. Make sure that your code can handle this issue. Also, the number of layers and hidden units, as well as the hyperparameters are up to you to figure out. \n",
    "\n",
    "Instructions on how to download these datasets are available in their [Github page](https://github.com/rois-codh/kmnist). To clone Kuzushiji repository from Github to Google Colab, use:\n",
    "\n",
    "!git clone https://github.com/rois-codh/kmnist\n",
    "\n",
    "Note: all Linux commands are available in Google Colab by preceding them with a \"!\", just like we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yT6izOPxOqU"
   },
   "outputs": [],
   "source": [
    "# Test your model using Kuzushiji-49 here\n",
    "print(neural_net(train_x_ros, train_y_nn, test_x, test_y_nn, learning_rate = 0.01, max_epochs = 100, pixel_dim = 784, classes = 49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy was 71.3% in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUya5ncqymwh"
   },
   "outputs": [],
   "source": [
    "# Test your model using Kuzushiji-Kanji here\n",
    "print(neural_net(kanji_train_x, kanji_train_y, kanj_test_x, kanji_test_y, learning_rate = 0.01, max_epochs = 100, pixel_dim = 4096, classes = 3832))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMDKQ9yWsS41"
   },
   "source": [
    "## Problem 2: Building a Decision Tree using Scikit-Learn's DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sli8nPfoz_0R"
   },
   "source": [
    "Repeat the above experiments using [Scikit-Learn's DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJSXe-szrWQk"
   },
   "outputs": [],
   "source": [
    "# Test your model using Kuzushiji-49 here\n",
    "dt = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 25)\n",
    "dt = dt.fit(train_x_ros, train_y_ros)\n",
    "pred = dt.predict(test_x)\n",
    "print(\"Accuracy:\", accuracy_score(pred, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy was 54% in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "waJGedMr4EJb"
   },
   "outputs": [],
   "source": [
    "# Test your model using Kuzushiji-Kanji here\n",
    "dt_kanji = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 25)\n",
    "dt_kanji = dt_kanji.fit(kanji_train_x, kanji_train_y)\n",
    "kanji_pred = dt_kanji.predict(kanji_train_x)\n",
    "print('Accuracy:', accuracy_score(kanji_pred, kanji_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qu_Y7_cesVdK"
   },
   "source": [
    "## Problem 3: Building a Gradient Boosted Tree using Tensorflow's BoostedTreeClassifier estimator\n",
    "\n",
    "In class we covered the vanilla decision tree. In recent years, a new kind of tree has become very popular in Kaggle competitions due to their performance, these trees are known as [Gradient Boosted Trees](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting). Typically, the winning submissions for most competitions make use of either some neural network or a gradient boosted tree (either, [XGBoost](https://xgboost.readthedocs.io/en/latest/) or [LightGBM](https://lightgbm.readthedocs.io/en/latest/)). For this problem, we will use [Tensorflow's BoostedTreeClassifier estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier). Repeat the above experiments using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGep6jYhsSCZ"
   },
   "outputs": [],
   "source": [
    "# Test your model using Kuzushiji-49 here\n",
    "params = {'objective' : 'multiclass', 'num_classes' : 49, 'learning_rate' : 0.001, 'bagging_fraction' : 0.5, 'bagging_freq' : 10}\n",
    "train_lgb = lgb.Dataset(train_x, label = train_y.ravel())\n",
    "\n",
    "lgbmodel = lgb.train(params, train_lgb, num_boost_round = 100)\n",
    "lgb_predict = lgbmodel.predict(train_y)\n",
    "\n",
    "print('accuracy:', accuracy_score(np.argmax(lgb_predict, axis = 1), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy was 67.01% in Google Colab with learning rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOc7_hTX4FuW"
   },
   "outputs": [],
   "source": [
    "# Test your model using Kuzushiji-Kanji here\n",
    "params = {'objective' : 'multiclass', 'num_classes' : 3832, 'learning_rate' : 0.001, 'bagging_fraction' : 0.25, 'bagging_freq' : 5}\n",
    "train_lgb = lgb.Dataset(kanji_train_x, label = kanji_train_y.ravel())\n",
    "\n",
    "lgbmodel = lgb.train(params, train_lgb, num_boost_round = 100)\n",
    "lgb_predict = lgbmodel.predict(kanji_test_x)\n",
    "\n",
    "print('accuracy:', accuracy_score(np.argmax(lgb_predict, axis = 1), kanji_test_y))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment3-P556-S19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
